{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e7c9692f-8ce4-4c2f-8a1f-2cfd03eb88d0",
   "metadata": {},
   "source": [
    "<img src=\"images/cs5228-header-title.png\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43445bb7",
   "metadata": {},
   "source": [
    "# Assignment 2 - Clustering & Association Rule Mining\n",
    "\n",
    "Hello everyone, this assignment notebook covers Clustering (again) and Association Rule Mining (ARM). There are some code-completion tasks and question-answering tasks in this answer sheet. For code completion tasks, please write down your answer (i.e., your lines of code) between sentences \"Your code starts here\" and \"Your code ends here\". The space between these two lines does not reflect the required or expected lines of code. For answers in plain text, you can refer to [this Markdown guide](https://medium.com/analytics-vidhya/the-ultimate-markdown-guide-for-jupyter-notebook-d5e5abf728fd) to customize the layout (although it shouldn't be needed).\n",
    "\n",
    "When you work on this notebook, you can insert additional code cells (e.g., for testing) or markdown cells (e.g., to keep track of your thoughts). However, before the submission, please remove all those additional cells again. Thanks!\n",
    "\n",
    "**Important:** \n",
    "* Remember to rename and save this Jupyter notebook as **A2_YourName_YourNUSNETID.ipynb** (e.g., **A2_BobSmith_e12345678.ipynb**) before submission!\n",
    "* Remember to rename and save the Python script file **A2_YourName_YourNUSNETID.py** (e.g., **A2_BobSmith_e12345678.py**) before submission!\n",
    "* Submission deadline is **Oct 3, 11.59 pm**. Late submissions will be penalized by 10% for each additional day. Failure to appropriately rename both files will yield a penalty of 1 Point. There is no need to use your full name if it's rather long; it's just important to easily identify you in Canvas etc.\n",
    "\n",
    "Please also add your NUSNET and student id in the code cell below. This is just to make any identification of your notebook doubly sure."
   ]
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-04T06:13:16.491546Z",
     "start_time": "2024-10-04T06:12:23.796828Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "url = \"https://nextcloud.wu.engineer/index.php/s/md5dL6XXjjBYBRG/download/endomondo_proper_cleaned_expanded.csv\"\n",
    "storage_options = {'User-Agent': 'Mozilla/5.0'}\n",
    "endomondo_df = pd.read_csv(url, storage_options=storage_options)"
   ],
   "id": "6dee83520824d357",
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "b6781ed8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:10:47.090774Z",
     "start_time": "2024-10-02T12:10:47.087740Z"
    }
   },
   "source": [
    "student_id = 'A0255954R'\n",
    "nusnet_id = 'e0962966'"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "id": "668b56a3",
   "metadata": {},
   "source": [
    "Here is an overview over the tasks to be solved and the points associated with each task. The notebook can appear very long and verbose, but note that a lot of parts provide additional explanations, documentation, or some discussion. The code and markdown cells you are supposed to complete are well-indicated, but you can use the overview below to double-check that you covered everything.\n",
    "\n",
    "* **1 DBSCAN & Comparing Cluster Algorithms (30 Points)**\n",
    "    * 1.1 Implementing DBSCAN for Noise Detection (10 Points)\n",
    "        * 1.1 a) Compute Core Points (5 Points)\n",
    "        * 1.2 b) Compute Noise Points (5 Points)\n",
    "    * 1.2 Questions about Clustering Algorithms (20 Points)\n",
    "        * 1.2 a) Interpreting Dendrograms for Hierarchical Clusterings (6 Points)\n",
    "        * 1.2 b) Comparing the Results of Different Clustering Algorithms (6 Points)\n",
    "        * 1.2 c) Short Essay Questions (8 Points)\n",
    "* **2 Association Rule Mining (ARM) (20 Points)**\n",
    "    * 2.1 Implementing Apriori Algorithm (10 Points)\n",
    "        * 2.1 a) Create Candidate Itemsets $L_k$ (6 Points)\n",
    "        * 2.1 b) Generate Frequent Itemsets with Apriori Algorithm (4 Points)\n",
    "    * 2.2 Recommending Movies using ARM (10 Points)\n",
    "        * 2.2 a) Compare the Runs A-D and Discuss your Observations! (3 Points) \n",
    "        * 2.2 b) Compare the Runs A-D and Discuss the Results for Building a Recommendation Engine! (3 Points)\n",
    "        * 2.2 c) Sketch a Movie Recommendation Algorithm Based on ARM (4 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "693531cc",
   "metadata": {},
   "source": [
    "## Setting up the Notebook"
   ]
  },
  {
   "cell_type": "code",
   "id": "e0483fca",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:10:47.155083Z",
     "start_time": "2024-10-02T12:10:47.146202Z"
    }
   },
   "source": [
    "# Some magic so that the notebook will reload the external python script file any time you edit and save the .py file;\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "id": "5875aa66",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:10:51.341328Z",
     "start_time": "2024-10-02T12:10:47.195663Z"
    }
   },
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "from efficient_apriori import apriori   # https://pypi.org/project/efficient-apriori/\n",
    "\n",
    "from src.utils import *\n",
    "\n",
    "np.set_printoptions(precision=2)"
   ],
   "outputs": [],
   "execution_count": 3
  },
  {
   "cell_type": "markdown",
   "id": "b583e6e9",
   "metadata": {},
   "source": [
    "**Important:** This notebook also requires you to complete in a separate `.py` script file. This keeps this notebook cleaner and simplifies testing your implementations for us. As you need to rename the file `A2_YourName_YourNUSNETID.py`, you also need to edit the import statement below accordingly."
   ]
  },
  {
   "cell_type": "code",
   "id": "6dea6158",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:10:51.641602Z",
     "start_time": "2024-10-02T12:10:51.514761Z"
    }
   },
   "source": [
    "from A2_WuTong_e0962966 import get_noise_dbscan\n",
    "#from A2_BobSmith_e12345678 import get_noise_dbscan # <-- you well need to rename this accordingly"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "cell_type": "markdown",
   "id": "4f3339fa",
   "metadata": {},
   "source": [
    "-------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a24c3c04-fdad-4c4d-ace7-cf0f05df823f",
   "metadata": {},
   "source": [
    "## 1 DBSCAN & Comparing Cluster Algorithms (30 Points)\n",
    "\n",
    "\n",
    "### 1.1 Implementing DBSCAN for Noise Detection (10 Points)\n",
    "\n",
    "In the lecture, we covered the original algorithm of DBSCAN, which you can also find on [Wikipedia](https://en.wikipedia.org/wiki/DBSCAN). While not difficult to implement, it takes quite a couple of lines of codes to do so. For this assignment, however, we are only interested in the points of a dataset that DBSCAN considers noise (as illustrated below; the red dots in the next plot). This includes that we do not have to care about\n",
    "\n",
    "* how many clusters there are (the plot below hints at 3 clusters but it does not matter) *and*\n",
    "* which non-noise points (the grey dots in the plot below) belong to which cluster\n",
    "\n",
    "**Your task is to implement a modified/simplified version of DBSCAN to find all noise points in a dataset!** The skeleton of method `get_noise_dbscan()` you need to complete is found in the file `A2.py` (before the appropriate renaming). The method takes data matrix `X` as well as the two basic parameters `eps` and `min_samples` as input parameters; we use the same naming as scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html).  The output should be 2 lists of indices: (a) one containing the indices of all *core points* and (b) one containing the indices of all *noise points* in input dataset X.\n",
    "\n",
    "**Important:**\n",
    "* We only split this task into 1.1 a) and 1.1 b) to have intermediate results you can check for correctness (and potentially to better allow for partial marking). Our reference solutions first finds all core points and uses this information to find all noise points; hence the 2 separate code blocks for you to complete.\n",
    "* However, if you have a better/faster/shorter/cooler/etc. solution, you are more than welcome to implement it and ignore the intermediate result of finding all core points. Only the result from 1.1 b) is important. This also means that you can ignore 1.1 a) and still get full marks if you correctly identify all noise points.\n",
    "* If you have an alternative solution, please make sure that the method still returns the 2 output parameters `(core_point_indices, noise_point_indices)`. If you do not need to explicitly identify the core points, you can simply return `None` for `core_point_indices`.\n",
    "* You can import any method `numpy`, `scipy`, `sklearn`, or `pandas` has to offer -- except for any ready-made implementation of DBSCAN, of course :). Please add any imports to the code cell at the top with the other imports. Hint: We already imported [`sklearn.metrics.pairwise.euclidean_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html) for you.\n",
    "\n",
    "We will benchmark your implementation as part of our Little Competitions to see whose solution is the fastest.\n",
    "\n",
    "#### Dataset Preparation (nothing for you to do here)"
   ]
  },
  {
   "cell_type": "code",
   "id": "b87b9ec5-ce07-43fa-942a-ab640ba4a693",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:10:54.003248Z",
     "start_time": "2024-10-02T12:10:53.988788Z"
    }
   },
   "source": [
    "X_dbscan_toy = pd.read_csv('data/a2-dbscan-toy-dataset.txt', header=None, sep=' ').to_numpy()\n",
    "\n",
    "print('The shape of X_dbscan_toy is {}'.format(X_dbscan_toy.shape))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The shape of X_dbscan_toy is (70, 2)\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "0b2afd7d-a291-4aa6-9ae5-691a0871d4fe",
   "metadata": {},
   "source": [
    "Now we can run scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) on this dataset. Here we use `eps=0.1` and `min_samples=10` as values for the two main input parameters for DBSCAN that specify the minimum \"density\" of clusters."
   ]
  },
  {
   "cell_type": "code",
   "id": "85ef8907-3b79-441a-b669-38748f6d4490",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:10:55.248410Z",
     "start_time": "2024-10-02T12:10:55.234487Z"
    }
   },
   "source": [
    "dbscan_clustering = DBSCAN(eps=0.1, min_samples=10).fit(X_dbscan_toy)"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "cell_type": "markdown",
   "id": "d7b8456d-799a-480d-add1-5ab4df59f690",
   "metadata": {},
   "source": [
    "The points that are noise points are labeled with `-1`, while all points belonging to clusters are labeled with `0`, `1`, `2`, etc. So we can easily find the indices of all the points labeled as noise as follows:"
   ]
  },
  {
   "cell_type": "code",
   "id": "1074f30e-0283-418c-8886-5d07cfcebce6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:10:56.214524Z",
     "start_time": "2024-10-02T12:10:56.201883Z"
    }
   },
   "source": [
    "cluster_point_indices = np.argwhere(dbscan_clustering.labels_ >= 0).squeeze()\n",
    "noise_point_indices = np.argwhere(dbscan_clustering.labels_ < 0).squeeze()\n",
    "\n",
    "print('The indices of the points labeled as noise are: {}'.format(noise_point_indices))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The indices of the points labeled as noise are: [ 0  4 27 31 33 39 43 46 51 65]\n"
     ]
    }
   ],
   "execution_count": 7
  },
  {
   "cell_type": "markdown",
   "id": "420e2f39-8a55-4aec-988f-6909b8f940df",
   "metadata": {},
   "source": [
    "Of course, we can also plot the results. Note that the figure below only highlights the points labeled as noise as red triangles; all points belonging to *some* clusters are in grey points (note that we do not care to which exact cluster these points belong)."
   ]
  },
  {
   "cell_type": "code",
   "id": "58486bb2-8993-40a7-b0fc-7962acc2705b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:10:57.772701Z",
     "start_time": "2024-10-02T12:10:57.678262Z"
    }
   },
   "source": [
    "plt.figure()\n",
    "plt.scatter(X_dbscan_toy[cluster_point_indices,0], X_dbscan_toy[cluster_point_indices,1], c='grey')\n",
    "plt.scatter(X_dbscan_toy[noise_point_indices,0], X_dbscan_toy[noise_point_indices,1], c='red', marker='^', s=75)\n",
    "plt.show()"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ],
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "6a3d72b4-fcc9-4576-9d9a-5737d452bbc0",
   "metadata": {},
   "source": [
    "Summing up, the red dots in the plots we define as noise or outliers as they are very dissimilar to the other data points. In practice, we would likely remove those noise points, treat them separately, or maybe perform additional preprocessing steps to potentially \"denoise\" the dataset. However, the steps of choice generally depend heavily on the exact data mining task. Here, we focus on the identification of noise points."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b59bf9e6-0662-4dd8-b138-c287e2cae0ac",
   "metadata": {},
   "source": [
    "#### 1.1 a) Compute Core Points (5 Points)\n",
    "\n",
    "As mentioned above, our reference solution first computes all core points. If you follow this approach, complete the respective part in the code of method `get_noise_dbscan()`. Some hints:\n",
    "* Recall that we do not care to which cluster a core point belongs to, only that it is a core point in *some* cluster\n",
    "* Have a look at method [`sklearn.metrics.pairwise.euclidean_distances`](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html); it might make your life easier."
   ]
  },
  {
   "cell_type": "code",
   "id": "163594f5-f327-4f26-8c8e-76ca798efd2b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:11:01.828908Z",
     "start_time": "2024-10-02T12:11:01.812278Z"
    }
   },
   "source": [
    "my_core_point_indices, _ = get_noise_dbscan(X_dbscan_toy, eps=0.1, min_samples=10)\n",
    "\n",
    "print('Total number of core points: {}\\n'.format(len(my_core_point_indices)))\n",
    "print('The first 25 indices of the points labeled as core points:\\n{}'.format(sorted(my_core_point_indices)[:20]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of core points: 50\n",
      "\n",
      "The first 25 indices of the points labeled as core points:\n",
      "[1, 3, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24]\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "cell_type": "markdown",
   "id": "441509bc-14f7-4c0e-bf41-944fd80f7c72",
   "metadata": {},
   "source": [
    "The output of previous code cell should look like:\n",
    "    \n",
    "```\n",
    "Total number of core points: 50\n",
    "\n",
    "The first 25 indices of the points labeled as core points:\n",
    "[1, 3, 5, 6, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 22, 23, 24]\n",
    "```\n",
    "\n",
    "Note that `0`, `4`, and `27` are missing from this list since [`sklearn.cluster.DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) told us that these points are noise. Of course, also the border points are missing here, but [`sklearn.cluster.DBSCAN`](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) does not return those explicitly."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b27639-e3f0-4667-be2b-b87870c98334",
   "metadata": {},
   "source": [
    "#### 1.1 b) Compute Noise Points (5 Points)\n",
    "\n",
    "Knowing the core points is useful but only an intermediate step. Now it is time to complete the method `get_noise_dbscan()` to compute the indices of all noise points in `X`. Again, our reference solution uses `core_point_indices` to accomplish this. If your implementation does not require the information about core points but returns the correct `noise_point_indices` then this is perfectly fine!"
   ]
  },
  {
   "cell_type": "code",
   "id": "dbea3a2c-84d5-4446-ad1b-e4610f52c87c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:11:03.660446Z",
     "start_time": "2024-10-02T12:11:03.647409Z"
    }
   },
   "source": [
    "_, my_noise_point_indices = get_noise_dbscan(X_dbscan_toy, eps=0.1, min_samples=10)\n",
    "\n",
    "print('Total number of noise points: {}\\n'.format(len(my_noise_point_indices)))\n",
    "print('The indices of all points labeled as noise points:\\n{}'.format(sorted(my_noise_point_indices)))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of noise points: 10\n",
      "\n",
      "The indices of all points labeled as noise points:\n",
      "[0, 4, 27, 31, 33, 39, 43, 46, 51, 65]\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "f2989aeb-280c-402c-8123-b246cce1c479",
   "metadata": {},
   "source": [
    "The output of previous code cell should look like:\n",
    "\n",
    "```\n",
    "Total number of noise points: 10\n",
    "\n",
    "The indices of all points labeled as noise points:\n",
    "[0, 4, 27, 31, 33, 39, 43, 46, 51, 65]\n",
    "```\n",
    "\n",
    "Since we used the same values for `eps` and `min_samples`, this result matches the output we saw earlier when we used scikit-learn's implementation of [DBSCAN](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.DBSCAN.html) over the toy dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82720640-0a91-4f47-aa5b-22f9372ce356",
   "metadata": {},
   "source": [
    "### 1.2 Questions about Clustering Algorithms (20 Points)\n",
    "\n",
    "#### 1.2 a) Interpreting Dendrograms for Hierarchical Clusterings (6 Points)\n",
    "\n",
    "We saw in the lecture that dendrograms are a meaningful way to visualize the hierarchical relationships between the data points with respect to the clustering using AGNES (or any other hierarchical clustering technique). Properly interpreting them is important to get a correct understanding of the underlying data.\n",
    "\n",
    "Below are the plots of 6 different datasets labeled A-F. Each dataset contains 30 data points, each with two dimensions.\n",
    "\n",
    "<img src=\"images/a2-agnes-data-labeled.png\">\n",
    "\n",
    "Below are 6 dendrograms labeled 1-6. These dendograms show the clustering using AGNES with Single Linkage for the 6 datasets above, but in a random order.\n",
    "\n",
    "<img src=\"images/a2-agnes-dendrogram-labeled.png\">\n",
    "\n",
    "Find the correct combinations of datasets and dendrograms -- that is, find for each dataset the dendrogram that visualizes the clustering using AGNES with Single Linkage! Give brief explanation for each decision! Complete the table below! (The last line shows an example.)\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f647bb9-7a94-437b-b62f-ed6bbc4b7f1c",
   "metadata": {},
   "source": [
    "| Dataset | Dendrogram                     | Brief Explanation                                                                                                                                                                                                                                                                         |\n",
    "| ---  |--------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **A**    | 6                              | This Dendrogram show there are two clusters formed at the same time and they have the similar node number as the merging goes. Then finally they merged into one cluster while the distance between two cluster is significantlly higher than others. This dendrogram fits the dataset A. |\n",
    "| **B**    | 5                              | For dendrogram 5, it has one node which has significant distance between others, and others have short distance but still can be merged to multiple cluster, which fit the dataset B                                                                                                      |\n",
    "| **C**    | 2                              | Dataset C shows there are 2 significant distanced node, and these 2 node must be merged from only 1 cluster but not multiple, so dendrogram 2 is fit for this pattern.                                                                                                                    |\n",
    "| **D**    | 4                              | Dataset D can show 3 cluster very clearly, then these 3 clusters merged to 1 big cluster but have larger distance. Which fit dendrogram 4.                                                                                                                                                |\n",
    "| **E**    | 1                              | Dataset E shows chaos and in the initial phase there could be lot of small clusters, and the final big cluster formation is not dramatic -- there is no ndoe leaves far away from others.                                                                                                 |\n",
    "| **F**    | 3                              | In the bottom of the dendrogram, there are many nodes merge to cluster, with the most nodes merged, the rest node's distance becomes larger and larger. This shows an 'expotenial increasing' which fit the dataset F.                                                                    |\n",
    "| **<font color='red'>X</font>**    | **<font color='red'>9</font>** | <font color='red'>The dataset plot looks like a face and the dendrogram looks like a hat (please come up with better explanations :) !)</font>                                                                                                                                            |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b91166-3520-43ed-8292-ea99890bec92",
   "metadata": {},
   "source": [
    "#### 1.2 b) Comparing the Results of Different Clustering Algorithms (6 Points)\n",
    "\n",
    "The figure belows shows the 6 different clusterings A-F, each computed over a dataset of 8 unique data points $x_1 x_2, ..., x_8$. The datasets are independent from each other for the 6 clusterings. Each clustering contains 3 clusters are represented by the table. A `1` in the result table indicates that the corresponding data point is part of the corresponding cluster. For example, in Clustering A, the `1` in the bottom-left cell indicates that data point $x_8$ is part of Cluster $C_1$.\n",
    "\n",
    "**Addtional constraints:**\n",
    "\n",
    "* For K-Means and DBSCAN, the 3 cluster $C_1$, $C_2$, and $C_3$ are the **only** clusters; for AGNES you can assume there might be **more** clusters in the hierarchy\n",
    "* For DBSCAN, the input parameter for the minimum number of neighboring points is  $MinPts \\geq 2$\n",
    "\n",
    "<img src=\"images/a2-clustering-comparison.png\">\n",
    "\n",
    "**For each clustering, decide which algorithm (K-Means, DBSCAN, AGNES) can have produced the clustering!** Use the table below for the answer. If an algorithm could have produced a clustering, just write *OK* in the respective cell of the table. If an algorithm could not have produced a clustering, enter a brief explanation into the respective table cell.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc323e0-ac0c-488c-9bc0-8c57e56cd370",
   "metadata": {},
   "source": [
    "|  | K-Means                                                        | DBSCAN                                                        | AGNES |\n",
    "|-----|----------------------------------------------------------------|---------------------------------------------------------------|-------|\n",
    "| **Clustering A**  | OK                                                             | OK                                                            | OK    |\n",
    "| **Clustering B**  | K-Means not allow mutiple cluster for one point                | DBSCAN not allow mutiple cluster for one point                | OK    |\n",
    "| **Clustering C**  | K-Means not allow no cluster or multiple cluster for one point | DBSCAN not allow multiple cluster for one point               | OK    |\n",
    "| **Clustering D**  | K-Means not allow no cluster for one point                     | OK                                                            | OK    |\n",
    "| **Clustering E**  | OK                                                             | Cluster 2 only has 1 point which below the min points setting | OK    |\n",
    "| **Clustering F**  | K-Means not allow mutiple cluster for one point                                                             | DBSCAN not allow mutiple cluster for one point                                                           | OK     |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffcb4d6c-5ba6-415b-8efa-9e62618f0441",
   "metadata": {},
   "source": [
    "#### 1.2 c) Short Essay Questions (8 Points)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b688752-fb83-4542-bc15-fe41794a3fb1",
   "metadata": {},
   "source": [
    "Recall the K-Means results in clusterings that are complete, i.e., each data point is assigned to a cluster. In contrast, DBSCAN has the notion of noise, i.e., points that are not part of any cluster, which can be used to identify outliers (see also Task 1.1). Now let's assume we want to identify outliers in a dataset but only have a standard implementation for K-Means available.\n",
    "\n",
    "**How can we utilize K-Means to (potentially) identify outliers? (2 Points)** Since the notion of outliers is not well defined, its not about having a fool-proof solution but to make a well-informed decision to limit the set of data points that are potential outliers.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "344ea51b-2e1d-4b49-9ecf-6dce70b63126",
   "metadata": {},
   "source": "To identify outliers using K-Means, we can calculate the Euclidean distance between each data point and its assigned cluster centroid. Points that are significantly farther from their centroids than the majority can be considered potential outliers. A threshold can be set based on the mean or standard deviation of these distances, and points that exceed this threshold can be flagged as outliers. This approach works on the assumption that outliers tend to have a larger distance from the cluster center compared to most other points."
  },
  {
   "cell_type": "markdown",
   "id": "ff20963e-f0e5-45d4-8ca8-b9c37c31fab1",
   "metadata": {},
   "source": [
    "We saw in the lecture that K-Means can return empty clusters.\n",
    "\n",
    "**In which situation may K-Means return at least 1 empty cluster? (2 Points)** To address this task, please make the following assumptions:\n",
    "* The number of data points $N$ in the dataset is $N > 0$\n",
    "* The number of clusters $K$ when running K-Means is $K \\geq 2$\n",
    "* The initialization of the initial centroids can be arbitrarily good or bad\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8301a44b-600f-4254-a771-5f598a0e77ea",
   "metadata": {},
   "source": "K-Means can return at least one empty cluster when, after an iteration, none of the data points are closest to a particular cluster centroid. This situation may arise if the initial centroids are poorly chosen or if the dataset is not evenly distributed. For example, if some centroids are initialized far from any dense group of data points, or if the centroids get updated to locations where no data points are nearby, no points will be assigned to those centroids, resulting in an empty cluster."
  },
  {
   "cell_type": "markdown",
   "id": "5b91a8af-651f-43f3-aef1-5f170bd345cb",
   "metadata": {},
   "source": [
    "Assume your dataset contains the geolocations of traffic accidents on Singapore expressways over the time span of a year. Using AGNES, you want to find sections of the expressways where traffic accidents are particularly common.\n",
    "\n",
    "**Which Linkage Methods covered in the lecture is most suitable for this task? (2 Points)** Briefly explain your choice!\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "523f1231-0d3e-41fa-a895-0ad5e1c41fcb",
   "metadata": {},
   "source": "The Complete Linkage method is the most suitable for this task. Complete linkage considers the maximum distance between points in different clusters, which helps to form compact and evenly distributed clusters. For traffic accident data, it is essential to identify sections of expressways where accidents are particularly common. Using complete linkage, we can identify clusters of accidents that are close to each other in space and time, which can help to identify high-risk areas on the expressways."
  },
  {
   "cell_type": "markdown",
   "id": "51c0fd71-2852-4be9-8749-37e7d7688a28",
   "metadata": {},
   "source": [
    "Assume you have a dataset `X`, run DBSCAN, and get a clustering that contains a set of clusters and some noise points (there's no need to be more precise; it's only important that you don't get just noise). Let's also assume you create a new dataset `X_new` simply by shuffling `X` (i.e., randomly change the order of data points in the dataset); no other changes. Now you run DBSCAN with the *same* parameters as before over `X_new` and get a different clustering, i.e., most of the clusters are not exactly the same as before.\n",
    "\n",
    "**What does this information tell about the dataset and clustering? (2 Points)** This may include a brief discussion how changing the parameters of DBSCAN will likely affect the results.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1679e54d-9e37-40bb-98c4-5aab965a08a1",
   "metadata": {},
   "source": [
    "The fact that shuffling the dataset X and running DBSCAN with the same parameters yields a different clustering suggests that the dataset may contain borderline points or areas of low density where the clustering is sensitive to small changes. This indicates that the clustering is unstable and that the dataset likely has regions where points are close to the decision boundary between being classified as part of a cluster or as noise.\n",
    "\n",
    "DBSCAN's results depend on the density of points in the neighborhood, and in datasets with borderline density regions, the inclusion or exclusion of points in clusters can vary. When parameters like eps (neighborhood radius) or min_samples (minimum number of points to form a core point) are near a critical threshold, small changes in point order can affect the clustering.\n",
    "\n",
    "To improve stability, increasing eps or decreasing min_samples might help by making the algorithm less sensitive to points near the boundary between clusters and noise. However, adjusting parameters can also cause over-clustering (merging separate clusters) or under-clustering (classifying too many points as noise)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2c755cb-3bc4-4428-b675-ed97bc22836d",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42ec0879",
   "metadata": {},
   "source": [
    "## 2 Association Rule Mining (ARM)\n",
    "\n",
    "Your task is to implement the Apriori Algorithm for finding Association Rules. In more detail, we focus on the **Apriori Algorithm for finding Frequent Itemsets** -- once we have the Frequent Itemsets, we use a naive approach for the association rule. We will provide a small method for that part later.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e6af0f",
   "metadata": {},
   "source": [
    "### 2.1 Implementing Apriori Algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c62cd6",
   "metadata": {},
   "source": [
    "#### Toy Dataset\n",
    "\n",
    "The following dataset with 5 transactions and 6 different items is directly taken from the lecture slides. This should make it easier to test your implementation. The format is a list of tuples, where each tuple represents the set of items of an individual transaction. This format can also be used as input for the `efficient-apriori` package."
   ]
  },
  {
   "cell_type": "code",
   "id": "a6b311d4",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:16:26.147911Z",
     "start_time": "2024-10-02T12:16:26.134695Z"
    }
   },
   "source": [
    "transactions_demo = [\n",
    "    ('bread', 'yogurt'),\n",
    "    ('bread', 'milk', 'cereal', 'eggs'),\n",
    "    ('yogurt', 'milk', 'cereal', 'cheese'),\n",
    "    ('bread', 'yogurt', 'milk', 'cereal'),\n",
    "    ('bread', 'yogurt', 'milk', 'cheese')\n",
    "]"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "cell_type": "markdown",
   "id": "bfe3ad90",
   "metadata": {},
   "source": [
    "#### Auxiliary Methods\n",
    "\n",
    "We want you to focus on the Apriori algorithm. So we provide a set of auxiliary functions. Feel free to look at their implementation in the file `src/utils.py`.\n",
    "\n",
    "The method `unique_items()` returns all the unique items across all transactions."
   ]
  },
  {
   "cell_type": "code",
   "id": "72077b2a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:16:27.295060Z",
     "start_time": "2024-10-02T12:16:27.277049Z"
    }
   },
   "source": [
    "unique_items(transactions_demo)"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bread', 'cereal', 'cheese', 'eggs', 'milk', 'yogurt'}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "cell_type": "markdown",
   "id": "629cf522",
   "metadata": {},
   "source": [
    "The method `support()` calculates and returns the support for a given itemset and set of transactions."
   ]
  },
  {
   "cell_type": "code",
   "id": "d86aeb90",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:16:28.107423Z",
     "start_time": "2024-10-02T12:16:28.092189Z"
    }
   },
   "source": [
    "support(transactions_demo, ('bread', 'milk'))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "markdown",
   "id": "0e0fce82",
   "metadata": {},
   "source": [
    "The method `confidence()` calculates and returns the confidence for a given association rule and set of transactions. An association rule is represented by a 2-tuple, where the first element represents itemset X and the second element represents items Y (i.e., $X \\Rightarrow Y$)"
   ]
  },
  {
   "cell_type": "code",
   "id": "7fa0e07a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:16:28.699749Z",
     "start_time": "2024-10-02T12:16:28.680460Z"
    }
   },
   "source": [
    "confidence(transactions_demo, (('bread',), ('milk',)))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.75"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "markdown",
   "id": "49823188",
   "metadata": {},
   "source": [
    "The method `merge_itemsets()` merges two given itemsets into one itemset."
   ]
  },
  {
   "cell_type": "code",
   "id": "a4cf8e68",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:16:29.260058Z",
     "start_time": "2024-10-02T12:16:29.243152Z"
    }
   },
   "source": [
    "merge_itemsets(('bread', 'milk'), ('bread', 'eggs'))"
   ],
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('bread', 'eggs', 'milk')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "markdown",
   "id": "8667ae69",
   "metadata": {},
   "source": [
    "For your implementation, you can make use of these auxiliary methods wherever you see fit. And that is, of course, strongly recommended, as it makes the programming task much easier. So, let's get started."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a49dbef4",
   "metadata": {},
   "source": [
    "#### 2.1 a) Create Candidate Itemsets $L_k$ (6 Points)\n",
    "\n",
    "Let's assume we have found $F_{k-1}$, i.e., all Frequent Itemsets for size $k-1$. For example $F_1$ is the set of all Frequent Itemsets of size 1, which is simply the set of unique items across all transactions with sufficient support. The next step is now to find $L_k$, all Candidate Itemsets of size $k$. In the lecture, we introduced two methods for this. For this assignment, we focus on the $\\mathbf{F_{k-1} \\times F_{k-1}}$ method -- that is, we use the Frequent Itemsets from the last step to calculate the Candidate Itemsets for the current step.\n",
    "\n",
    "Recall from the lecture that creating $L_k$ involves two main parts:\n",
    "\n",
    "* **Generating** all possible $k$-itemsets from the Frequent Itemsets $F_{k-1}$; and\n",
    "\n",
    "* **Pruning** all $k$-itemsets that cannot be frequent based on the information we already have ($L_k$ should only contain the itemsets for which we indeed calculate the support for)\n",
    "\n",
    "\n",
    "Recall that we also can (and should) **prune** any Candidate Itemsets than cannot possibly also be Frequent Itemsets  based on the information we already have. In other words, the Candidate Itemsets of size $k$ should only contain the itemsets for which we indeed calculate the support for.\n",
    "\n",
    "**Hint:** In the lecture, to make it more illustrative, we first generate all possible Candidate Itemsets and then prune the ones that cannot possibly be frequent. In practice, to save memory space, it's better to check each Candidate Itemset immediately before even adding it to $L_k$. The skeleton code below reflects this. However, if you indeed want to implement pruning as its own step, you're free to do so.\n",
    "\n",
    "**Implement method `generate_Lk()` to calculate the Candidate Itemsets $L_k$ given the Frequent Itemsets $F_{k-1}$!** Note that we walked in detail through an example of this process in the lecture. Below is a code cell that reflects this example to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "id": "6413261c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:16:30.540387Z",
     "start_time": "2024-10-02T12:16:30.527892Z"
    }
   },
   "source": [
    "def generate_Lk(Fk_minus_one):\n",
    "\n",
    "    # The code just looks a bit odd since we cannot get an element from a set using indexing\n",
    "    k = len(next(iter(Fk_minus_one))) + 1\n",
    "\n",
    "    # Initialize as set as a fail safe to avoid any duplicates\n",
    "    Lk = set()\n",
    "    \n",
    "    for itemset1 in Fk_minus_one:\n",
    "        for itemset2 in Fk_minus_one:\n",
    "            \n",
    "            ######################################################################\n",
    "            ### Your code starts here ############################################\n",
    "            \n",
    "            # Merge the two itemsets\n",
    "            candidate_itemset = merge_itemsets(itemset1, itemset2)\n",
    "            if len(candidate_itemset) == k:\n",
    "                Lk.add(candidate_itemset)\n",
    "\n",
    "            \n",
    "            ### Your code ends here ##############################################\n",
    "            ######################################################################\n",
    "            \n",
    "            pass # Just there so the empty loop does not throw an error\n",
    "    \n",
    "    ######################################################################\n",
    "    ### Your code starts here ############################################\n",
    "    \n",
    "    # MAY ONLY BE REQUIRED IF YOU TREAT PRUNING AS A SEPARATE STEP!!!\n",
    "    # which you shouldn't for performance reasons in practice, but for the assignment its fine\n",
    "    \n",
    "    ### Your code ends here ##############################################\n",
    "    ######################################################################\n",
    "    \n",
    "    return Lk"
   ],
   "outputs": [],
   "execution_count": 20
  },
  {
   "cell_type": "code",
   "id": "45e92ac0",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:16:31.084681Z",
     "start_time": "2024-10-02T12:16:31.071951Z"
    }
   },
   "source": [
    "k_itemsets = generate_Lk({\n",
    "    ('bread', 'cereal'), ('bread', 'milk'), ('bread', 'yogurt'), ('cereal', 'milk'),\n",
    "    ('cereal', 'yogurt'), ('cheese', 'milk'), ('cheese', 'yogurt'), ('milk', 'yogurt')\n",
    "})\n",
    "\n",
    "for itemset in k_itemsets:\n",
    "    print(itemset)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('bread', 'cereal', 'yogurt')\n",
      "('cereal', 'cheese', 'milk')\n",
      "('cheese', 'milk', 'yogurt')\n",
      "('cereal', 'milk', 'yogurt')\n",
      "('bread', 'cheese', 'yogurt')\n",
      "('bread', 'cereal', 'milk')\n",
      "('bread', 'milk', 'yogurt')\n",
      "('cereal', 'cheese', 'yogurt')\n",
      "('bread', 'cheese', 'milk')\n"
     ]
    }
   ],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "8c7ce928",
   "metadata": {},
   "source": [
    "#### 2.1 b) Generate Frequent Itemsets with Apriori Algorithm (4 Points)\n",
    "\n",
    "The method `generate_Lk()` covered the \"Generate\" and \"Prune\" steps of the Apriori Algorithm for finding Frequent Itemsets. Now only the \"Calculate\" and \"Filter\" step is missing. However, with `generate_Lk()` in place and together with the auxiliary methods we provide (see above), putting the Apriori Algorithm together should be pretty straightforward.\n",
    "\n",
    "**Implement `frequent_itemsets_apriori()` to find all Frequent Itemset given a set of transactions and a minimum support of `min_support`!** Again, below is a code cell that reflects this example to test your implementation."
   ]
  },
  {
   "cell_type": "code",
   "id": "63b0a0dd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:16:32.385082Z",
     "start_time": "2024-10-02T12:16:32.372272Z"
    }
   },
   "source": [
    "def frequent_itemsets_apriori(transactions, min_support):\n",
    "    \n",
    "    # The frequent 1-itemsets are all unique items across all transactions with sufficient support\n",
    "    # The one-liner below simply loops over all uniques items and checks the condition w.r.t. the support\n",
    "    F1 = set([(s,) for s in unique_items(transactions) if support(transactions, (s,)) >= min_support ])\n",
    "    \n",
    "    # If there is not even a single 1-itemset that is frequent, we can just stop here\n",
    "    if len(F1) == 0:\n",
    "        return {}\n",
    "    \n",
    "    # Initialize dictionary with all current frequent itemsets for each size k\n",
    "    # Example: { 1: {(a), (b), (c)}, 2: {(a, c), ...} }\n",
    "    F = { 1: F1 }\n",
    "    \n",
    "    # Find now all frequent itemsets of size 2, 3, 4, ... (sys.maxsize basically mean infinity here)\n",
    "    for k in range(2, sys.maxsize):\n",
    "\n",
    "        Fk = set()\n",
    "        \n",
    "        ########################################################################################\n",
    "        ### Your code starts here ##############################################################\n",
    "\n",
    "        # Generate the Candidate Itemsets Lk\n",
    "        Lk = generate_Lk(F[k-1])\n",
    "        Fk = set([ itemset for itemset in Lk if support(transactions, itemset) >= min_support ])\n",
    "        if len(Fk) == 0:\n",
    "            break\n",
    "        \n",
    "        ### Your code ends here ################################################################\n",
    "        ########################################################################################\n",
    "                \n",
    "        F[k] = Fk    \n",
    "\n",
    "    # Merge the dictionary of itemsets to a single set and return it\n",
    "    # Example: {1: {(a), (b), (c)}, 2: (a, c)} => {(a), (b), (c), (a, c)}\n",
    "    return set.union(*[ itemsets for k, itemsets in F.items() ])"
   ],
   "outputs": [],
   "execution_count": 22
  },
  {
   "cell_type": "code",
   "id": "5ce7be60",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:16:32.966493Z",
     "start_time": "2024-10-02T12:16:32.953052Z"
    }
   },
   "source": [
    "frequent_itemsets = frequent_itemsets_apriori(transactions_demo, 0.6)\n",
    "for itemset in frequent_itemsets:\n",
    "    print(itemset)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('cereal', 'milk')\n",
      "('cereal',)\n",
      "('yogurt',)\n",
      "('bread', 'yogurt')\n",
      "('milk',)\n",
      "('milk', 'yogurt')\n",
      "('bread',)\n",
      "('bread', 'milk')\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "markdown",
   "id": "1c5d0094",
   "metadata": {},
   "source": [
    "#### From Frequent Itemsets to Association Rules (nothing for you to do here!)\n",
    "\n",
    "Your implementation so far gives you the Frequent Itemsets in a list of transactions using the Apriori method. This step is typically the most time-consuming one in Association Rule Mining. However, we still have to do the second step and find all Association Rules given the Frequent Itemsets. We saw in the lecture that this can also be done in an efficient manner using the Apriori method to avoid checking all rules.\n",
    "\n",
    "Since this step is typically less computationally expensive, we simply do it the naive way -- that is, we go over all Frequent Itemsets, and check for each Frequent Itemset and which of the Association Rules that can be generated from it has a sufficiently high confidence. With all the auxiliary methods we provide, this becomes trivial to implement, so we simply give you the method `find_association_rules()` below. Note how it uses your implementation of `frequent_itemsets_apriori()`."
   ]
  },
  {
   "cell_type": "code",
   "id": "6abc8144",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:16:54.316764Z",
     "start_time": "2024-10-02T12:16:54.301030Z"
    }
   },
   "source": [
    "def find_association_rules(transactions, min_support, min_confidence):\n",
    "    # Initialize empty list of association rules\n",
    "    association_rules = []\n",
    "    \n",
    "    # Find and loop over all frequent itemsets\n",
    "    for itemset in frequent_itemsets_apriori(transactions, min_support):\n",
    "        if len(itemset) == 1:\n",
    "            continue\n",
    "\n",
    "        # Find and loop over all association rules that can be generated from the itemset\n",
    "        for r in generate_association_rules(itemset):\n",
    "            # Check if the association rule fulfils the confidence requriement\n",
    "            if confidence(transactions, r) >= min_confidence:\n",
    "                association_rules.append(r)\n",
    "                \n",
    "    # Return final list of association rules\n",
    "    return association_rules"
   ],
   "outputs": [],
   "execution_count": 24
  },
  {
   "cell_type": "code",
   "id": "e2ba7b23",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:16:54.734024Z",
     "start_time": "2024-10-02T12:16:54.722034Z"
    }
   },
   "source": [
    "for rule in find_association_rules(transactions_demo, 0.4, 1.0):\n",
    "    print(rule)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(('cheese',), ('yogurt',))\n",
      "(('cereal',), ('milk',))\n",
      "(('cheese',), ('milk',))\n",
      "(('cheese',), ('milk', 'yogurt'))\n",
      "(('cheese', 'milk'), ('yogurt',))\n",
      "(('cheese', 'yogurt'), ('milk',))\n",
      "(('cereal', 'yogurt'), ('milk',))\n",
      "(('bread', 'cereal'), ('milk',))\n"
     ]
    }
   ],
   "execution_count": 25
  },
  {
   "cell_type": "markdown",
   "id": "88167699",
   "metadata": {},
   "source": [
    "#### Comparison with `efficient-apriori` package  (nothing for you to do here!)\n",
    "\n",
    "You can run the apriori algorithm over the demo data to check if your implementation is correct. Try different values for the parameters `min_support` and `min_confidence` and compare the results. Note that the order of the returned association rules might differ between your implementation and the apriori one."
   ]
  },
  {
   "cell_type": "code",
   "id": "62eee91f",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:17:02.121089Z",
     "start_time": "2024-10-02T12:17:02.104155Z"
    }
   },
   "source": [
    "_, rules = apriori(transactions_demo, min_support=0.4, min_confidence=1.0)\n",
    "\n",
    "for r in rules:\n",
    "    print('Rule [{} => {}] (support: {}, confidence: {}, lift: {})'.format(r.lhs, r.rhs, r.support, r.confidence, r.lift))\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rule [('cereal',) => ('milk',)] (support: 0.6, confidence: 1.0, lift: 1.25)\n",
      "Rule [('cheese',) => ('milk',)] (support: 0.4, confidence: 1.0, lift: 1.25)\n",
      "Rule [('cheese',) => ('yogurt',)] (support: 0.4, confidence: 1.0, lift: 1.25)\n",
      "Rule [('bread', 'cereal') => ('milk',)] (support: 0.4, confidence: 1.0, lift: 1.25)\n",
      "Rule [('cereal', 'yogurt') => ('milk',)] (support: 0.4, confidence: 1.0, lift: 1.25)\n",
      "Rule [('cheese', 'yogurt') => ('milk',)] (support: 0.4, confidence: 1.0, lift: 1.25)\n",
      "Rule [('cheese', 'milk') => ('yogurt',)] (support: 0.4, confidence: 1.0, lift: 1.25)\n",
      "Rule [('cheese',) => ('milk', 'yogurt')] (support: 0.4, confidence: 1.0, lift: 1.6666666666666667)\n"
     ]
    }
   ],
   "execution_count": 26
  },
  {
   "cell_type": "markdown",
   "id": "97a70dbc",
   "metadata": {},
   "source": [
    "The `efficient-apriori` provides, of course, a much more efficient and convenient implementation (e.g., keeping track of all the metrics for each rule). And this is why we use this package for finding Association Rules in a real-world dataset below. Still, in its core, `efficient-apriori` implements the same underlying Apriori method to Find Frequent Itemsets (but also to find the Association Rules). If you're interested, further below, you can compare the runtimes of `efficient-apriori` and your implementation. Just don't be too disappointed :)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6443ffda",
   "metadata": {},
   "source": [
    "### 2.2 Recommending Movies using ARM\n",
    "\n",
    "In this task, we look into using Association Rule Mining for recommending movies -- more specifically, recommending movies on physical mediums (Blu-ray, DVD, etc.), assuming that is still a thing nowadays :).\n",
    "\n",
    "**Dataset.** E-commerce sites do not really make their data publicly available, so we do not have any hard real-world dataset. For the context of this assignment, this is of course no problem. What we use here is a popular movie ratings dataset from [MovieLens](https://grouplens.org/datasets/movielens/). This dataset contains user ratings for movies (1-5 stars, incl. half stars, e.g., 3.5). More specifically, we use the [MovieLens 1M Dataset](https://grouplens.org/datasets/movielens/1m/) containing 1 Million ratings from ~6,000 users on ~4,000 movies and was released February 2003 -- so do not expect any recent Marvel movies :).\n",
    "\n",
    "While using these ratings allow for more sophisticated recommendation algorithms -- and we will look into some of those in a later lecture -- here we are focusing on Association Rules. This includes that we need to convert this rating dataset into a transaction dataset, where a transaction represents all the movies a user has purchased. We already did this for you making the following assumption: A User has purchased all the movies s/he gave the highest rating. For example, if User A gave a highest rating of 4.5 to any movie, A has purchased all movies A rated with 4.5. This is certainly a simplifying assumption, but perfectly fine for this task here.\n",
    "\n",
    "Let's have a quick look at the data. First, we load the ids and names of all movies into a dictionary. We need this dictionary since our transactions (i.e., the list of movies a user has bought) contains the ids and not the names of the movies. So to actually see the names of movies in the association rules, we need this way to map from a movie's id to its name."
   ]
  },
  {
   "cell_type": "code",
   "id": "d934b1eb",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:17:13.655485Z",
     "start_time": "2024-10-02T12:17:13.633594Z"
    }
   },
   "source": [
    "# Read file with movies (and der ids) into a pandas dataframe\n",
    "df_movies = pd.read_csv('data/a2-arm-movies.csv', header=None)\n",
    "# Convert dataframe to dictionary for quick lookups\n",
    "movie_map = dict(zip(df_movies[0], df_movies[1]))\n",
    "# Show the first 5 entries as example\n",
    "for movie_id, movie_name in movie_map.items():\n",
    "    print('{} -> {}'.format(movie_id, movie_name))\n",
    "    if movie_id >= 5:\n",
    "        break"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 -> Toy Story\n",
      "2 -> Jumanji\n",
      "3 -> Grumpier Old Men\n",
      "4 -> Waiting to Exhale\n",
      "5 -> Father of the Bride Part II\n"
     ]
    }
   ],
   "execution_count": 27
  },
  {
   "cell_type": "markdown",
   "id": "4e553756",
   "metadata": {},
   "source": [
    "No we can load the transactions. Again, a transaction is a user's shopping history, i.e., all the movies the user has bought. "
   ]
  },
  {
   "cell_type": "code",
   "id": "c3717ccd",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:17:17.552014Z",
     "start_time": "2024-10-02T12:17:17.511187Z"
    }
   },
   "source": [
    "shopping_histories = []\n",
    "\n",
    "# Read shopping histories; each line is a comma-separated list of the movies (i.e., their ids!) a user bought\n",
    "with open('data/a2-arm-movie-shopping-histories.csv') as file:\n",
    "    for line in file:\n",
    "        shopping_histories.append(tuple([ int(i) for i in line.strip().split(',') ]))\n",
    "\n",
    "# Show the shopping history of the first user for an example; we need movie_map to get the name of each movie\n",
    "user = 0\n",
    "\n",
    "print('Shopping history for user {} (used for Aprior algorithm)'.format(user))\n",
    "print(shopping_histories[user])\n",
    "print()\n",
    "print('Detailed shopping history for user {}'.format(user))\n",
    "for movie_id in shopping_histories[user]:\n",
    "    print('{}: {}'.format(movie_id, movie_map[movie_id]))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shopping history for user 0 (used for Aprior algorithm)\n",
      "(1, 48, 150, 527, 595, 1022, 1028, 1029, 1035, 1193, 1270, 1287, 1836, 1961, 2028, 2355, 2804, 3105)\n",
      "\n",
      "Detailed shopping history for user 0\n",
      "1: Toy Story\n",
      "48: Pocahontas\n",
      "150: Apollo 13\n",
      "527: Schindler's List\n",
      "595: Beauty and the Beast\n",
      "1022: Cinderella\n",
      "1028: Mary Poppins\n",
      "1029: Dumbo\n",
      "1035: Sound of Music, The\n",
      "1193: One Flew Over the Cuckoo's Nest\n",
      "1270: Back to the Future\n",
      "1287: Ben\n",
      "1836: Last Days of Disco, The\n",
      "1961: Rain Man\n",
      "2028: Saving Private Ryan\n",
      "2355: Bug's Life, A\n",
      "2804: Christmas Story, A\n",
      "3105: Awakenings\n"
     ]
    }
   ],
   "execution_count": 28
  },
  {
   "cell_type": "markdown",
   "id": "79366a78",
   "metadata": {},
   "source": [
    "With the dataset loaded, we are ready to find interesting Association Rules. For performance reasons, we use the `efficient_apriori` package -- however, further below there is an optional code cell where you can use your own implementation of the Apriori algorithm, in case you are interested.\n",
    "\n",
    "For added convenience, we provide method `show_top_rules()` which computes the Association Rules using the `efficient-apriori` package, but (a) sorts the rules w.r.t. the specified metric (default: lift), and (b) shows only the top-k rules (default: 5). The method also ensures a consistent output of each Association Rule. Each rule contains the LHS, RHS, as well as the support (s), confidence (c), and lift (l). Feel free to check out the code of method `show_top_rules()` in `src.utils` if anything might be unclear regarding its use.\n",
    "\n",
    "**Run the following 4 code cells and interpret the results below!** All 4 code cells find Association Rules using the `efficient-apriori` package encapsulated in the auxiliary method `show_top_rules()` for convenience. Appreciate how Runs A-B differ with respect to the input parameter of the method calls! Also, note that we call `show_top_rules()` with `id_map=None` at first, so the results will only display the movie ids. Later, you will be asked to run the cells again with `id_map=movie_map` to see the actual names of the movies."
   ]
  },
  {
   "cell_type": "code",
   "id": "f2d64405",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:19:02.628975Z",
     "start_time": "2024-10-02T12:19:02.516486Z"
    }
   },
   "source": [
    "%%time\n",
    "# Run A\n",
    "show_top_rules(shopping_histories, min_support=0.1, min_confidence=0.2, k=10, id_map=None)\n",
    "show_top_rules(shopping_histories, min_support=0.1, min_confidence=0.2, k=10, id_map=movie_map)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Number of Rules: 68 ===\n",
      "(1221) => (858)  [s: 0.13, c: 0.86, l: 3.51]\n",
      "(858) => (1221)  [s: 0.13, c: 0.55, l: 3.51]\n",
      "(260; 1210) => (1196)  [s: 0.11, c: 0.84, l: 3.43]\n",
      "(1196) => (260; 1210)  [s: 0.11, c: 0.44, l: 3.43]\n",
      "(260; 1196) => (1210)  [s: 0.11, c: 0.58, l: 3.42]\n",
      "(1210) => (260; 1196)  [s: 0.11, c: 0.63, l: 3.42]\n",
      "(1210) => (1196)  [s: 0.12, c: 0.73, l: 2.96]\n",
      "(1196) => (1210)  [s: 0.12, c: 0.51, l: 2.96]\n",
      "(260; 1198) => (1196)  [s: 0.11, c: 0.71, l: 2.88]\n",
      "(1196) => (260; 1198)  [s: 0.11, c: 0.46, l: 2.88]\n",
      "(1196; 1210) => (260)  [s: 0.11, c: 0.87, l: 2.86]\n",
      "\n",
      "=== Total Number of Rules: 68 ===\n",
      "(Godfather: Part II, The) => (Godfather, The)  [s: 0.13, c: 0.86, l: 3.51]\n",
      "(Godfather, The) => (Godfather: Part II, The)  [s: 0.13, c: 0.55, l: 3.51]\n",
      "(Star Wars: Episode IV; Star Wars: Episode VI) => (Star Wars: Episode V)  [s: 0.11, c: 0.84, l: 3.43]\n",
      "(Star Wars: Episode V) => (Star Wars: Episode IV; Star Wars: Episode VI)  [s: 0.11, c: 0.44, l: 3.43]\n",
      "(Star Wars: Episode IV; Star Wars: Episode V) => (Star Wars: Episode VI)  [s: 0.11, c: 0.58, l: 3.42]\n",
      "(Star Wars: Episode VI) => (Star Wars: Episode IV; Star Wars: Episode V)  [s: 0.11, c: 0.63, l: 3.42]\n",
      "(Star Wars: Episode VI) => (Star Wars: Episode V)  [s: 0.12, c: 0.73, l: 2.96]\n",
      "(Star Wars: Episode V) => (Star Wars: Episode VI)  [s: 0.12, c: 0.51, l: 2.96]\n",
      "(Star Wars: Episode IV; Raiders of the Lost Ark) => (Star Wars: Episode V)  [s: 0.11, c: 0.71, l: 2.88]\n",
      "(Star Wars: Episode V) => (Star Wars: Episode IV; Raiders of the Lost Ark)  [s: 0.11, c: 0.46, l: 2.88]\n",
      "(Star Wars: Episode V; Star Wars: Episode VI) => (Star Wars: Episode IV)  [s: 0.11, c: 0.87, l: 2.86]\n",
      "\n",
      "CPU times: user 96.9 ms, sys: 1.95 ms, total: 98.8 ms\n",
      "Wall time: 98.1 ms\n"
     ]
    }
   ],
   "execution_count": 34
  },
  {
   "cell_type": "code",
   "id": "8b75ea14",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:20:33.128009Z",
     "start_time": "2024-10-02T12:19:09.440549Z"
    }
   },
   "source": [
    "%%time\n",
    "# Run B\n",
    "show_top_rules(shopping_histories, min_support=0.01, min_confidence=0.2, k=10, id_map=None)\n",
    "show_top_rules(shopping_histories, min_support=0.01, min_confidence=0.2, k=10, id_map=movie_map)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Number of Rules: 2962203 ===\n",
      "(1132) => (1131)  [s: 0.01, c: 0.64, l: 35.18]\n",
      "(1131) => (1132)  [s: 0.01, c: 0.60, l: 35.18]\n",
      "(1148; 1221) => (745; 858)  [s: 0.01, c: 0.60, l: 28.40]\n",
      "(745; 858) => (1148; 1221)  [s: 0.01, c: 0.48, l: 28.40]\n",
      "(260; 1148; 1196) => (745; 1210)  [s: 0.01, c: 0.39, l: 27.32]\n",
      "(745; 1210) => (260; 1148; 1196)  [s: 0.01, c: 0.70, l: 27.32]\n",
      "(260; 745; 1196) => (1148; 1210)  [s: 0.01, c: 0.49, l: 27.29]\n",
      "(1148; 1210) => (260; 745; 1196)  [s: 0.01, c: 0.56, l: 27.29]\n",
      "(858; 1148) => (745; 1221)  [s: 0.01, c: 0.37, l: 26.54]\n",
      "(745; 1221) => (858; 1148)  [s: 0.01, c: 0.73, l: 26.54]\n",
      "(260; 745; 1148) => (1196; 1223)  [s: 0.01, c: 0.39, l: 25.65]\n",
      "\n",
      "=== Total Number of Rules: 2962203 ===\n",
      "(Manon of the Spring) => (Jean de Florette)  [s: 0.01, c: 0.64, l: 35.18]\n",
      "(Jean de Florette) => (Manon of the Spring)  [s: 0.01, c: 0.60, l: 35.18]\n",
      "(Wrong Trousers, The; Godfather: Part II, The) => (Close Shave, A; Godfather, The)  [s: 0.01, c: 0.60, l: 28.40]\n",
      "(Close Shave, A; Godfather, The) => (Wrong Trousers, The; Godfather: Part II, The)  [s: 0.01, c: 0.48, l: 28.40]\n",
      "(Star Wars: Episode IV; Wrong Trousers, The; Star Wars: Episode V) => (Close Shave, A; Star Wars: Episode VI)  [s: 0.01, c: 0.39, l: 27.32]\n",
      "(Close Shave, A; Star Wars: Episode VI) => (Star Wars: Episode IV; Wrong Trousers, The; Star Wars: Episode V)  [s: 0.01, c: 0.70, l: 27.32]\n",
      "(Star Wars: Episode IV; Close Shave, A; Star Wars: Episode V) => (Wrong Trousers, The; Star Wars: Episode VI)  [s: 0.01, c: 0.49, l: 27.29]\n",
      "(Wrong Trousers, The; Star Wars: Episode VI) => (Star Wars: Episode IV; Close Shave, A; Star Wars: Episode V)  [s: 0.01, c: 0.56, l: 27.29]\n",
      "(Godfather, The; Wrong Trousers, The) => (Close Shave, A; Godfather: Part II, The)  [s: 0.01, c: 0.37, l: 26.54]\n",
      "(Close Shave, A; Godfather: Part II, The) => (Godfather, The; Wrong Trousers, The)  [s: 0.01, c: 0.73, l: 26.54]\n",
      "(Star Wars: Episode IV; Close Shave, A; Wrong Trousers, The) => (Star Wars: Episode V; Grand Day Out, A)  [s: 0.01, c: 0.39, l: 25.65]\n",
      "\n",
      "CPU times: user 1min 22s, sys: 927 ms, total: 1min 23s\n",
      "Wall time: 1min 23s\n"
     ]
    }
   ],
   "execution_count": 35
  },
  {
   "cell_type": "code",
   "id": "563ec2f6",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:20:33.238874Z",
     "start_time": "2024-10-02T12:20:33.132364Z"
    }
   },
   "source": [
    "%%time\n",
    "# Run C\n",
    "show_top_rules(shopping_histories, min_support=0.1, min_confidence=0.8, k=10, reverse=True, id_map=None)\n",
    "show_top_rules(shopping_histories, min_support=0.1, min_confidence=0.8, k=10, reverse=True, id_map=movie_map)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Number of Rules: 4 ===\n",
      "(1221) => (858)  [s: 0.13, c: 0.86, l: 3.51]\n",
      "(260; 1210) => (1196)  [s: 0.11, c: 0.84, l: 3.43]\n",
      "(1196; 1210) => (260)  [s: 0.11, c: 0.87, l: 2.86]\n",
      "(1196; 1198) => (260)  [s: 0.11, c: 0.84, l: 2.76]\n",
      "\n",
      "=== Total Number of Rules: 4 ===\n",
      "(Godfather: Part II, The) => (Godfather, The)  [s: 0.13, c: 0.86, l: 3.51]\n",
      "(Star Wars: Episode IV; Star Wars: Episode VI) => (Star Wars: Episode V)  [s: 0.11, c: 0.84, l: 3.43]\n",
      "(Star Wars: Episode V; Star Wars: Episode VI) => (Star Wars: Episode IV)  [s: 0.11, c: 0.87, l: 2.86]\n",
      "(Star Wars: Episode V; Raiders of the Lost Ark) => (Star Wars: Episode IV)  [s: 0.11, c: 0.84, l: 2.76]\n",
      "\n",
      "CPU times: user 92.6 ms, sys: 2.2 ms, total: 94.8 ms\n",
      "Wall time: 94.2 ms\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "cell_type": "code",
   "id": "ee231d29",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:21:35.041001Z",
     "start_time": "2024-10-02T12:20:33.259100Z"
    }
   },
   "source": [
    "%%time\n",
    "# Run D\n",
    "show_top_rules(shopping_histories, min_support=0.01, min_confidence=0.8, k=10, reverse=True, id_map=None)\n",
    "show_top_rules(shopping_histories, min_support=0.01, min_confidence=0.8, k=10, reverse=True, id_map=movie_map)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Total Number of Rules: 122486 ===\n",
      "(745; 1196; 1223) => (260; 1148)  [s: 0.01, c: 0.84, l: 21.17]\n",
      "(745; 1196; 1198) => (260; 1148)  [s: 0.01, c: 0.83, l: 20.89]\n",
      "(745; 1196; 1210) => (260; 1148)  [s: 0.01, c: 0.81, l: 20.55]\n",
      "(720; 1223) => (745; 1148)  [s: 0.01, c: 0.82, l: 15.70]\n",
      "(2571; 2951) => (1201)  [s: 0.01, c: 0.83, l: 15.44]\n",
      "(1; 1223) => (745; 1148)  [s: 0.01, c: 0.80, l: 15.39]\n",
      "(1089; 1196; 1221; 2858) => (260; 296; 858)  [s: 0.01, c: 0.80, l: 15.39]\n",
      "(1089; 1196; 1198; 1221) => (260; 296; 858)  [s: 0.01, c: 0.80, l: 15.34]\n",
      "(260; 1196; 2951) => (1201)  [s: 0.01, c: 0.82, l: 15.27]\n",
      "(858; 2951) => (1201)  [s: 0.01, c: 0.81, l: 15.07]\n",
      "(1196; 2951) => (1201)  [s: 0.01, c: 0.81, l: 14.96]\n",
      "\n",
      "=== Total Number of Rules: 122486 ===\n",
      "(Close Shave, A; Star Wars: Episode V; Grand Day Out, A) => (Star Wars: Episode IV; Wrong Trousers, The)  [s: 0.01, c: 0.84, l: 21.17]\n",
      "(Close Shave, A; Star Wars: Episode V; Raiders of the Lost Ark) => (Star Wars: Episode IV; Wrong Trousers, The)  [s: 0.01, c: 0.83, l: 20.89]\n",
      "(Close Shave, A; Star Wars: Episode V; Star Wars: Episode VI) => (Star Wars: Episode IV; Wrong Trousers, The)  [s: 0.01, c: 0.81, l: 20.55]\n",
      "(Wallace & Gromit: The Best of Aardman Animation; Grand Day Out, A) => (Close Shave, A; Wrong Trousers, The)  [s: 0.01, c: 0.82, l: 15.70]\n",
      "(Matrix, The; Fistful of Dollars, A) => (Good, The Bad and The Ugly, The)  [s: 0.01, c: 0.83, l: 15.44]\n",
      "(Toy Story; Grand Day Out, A) => (Close Shave, A; Wrong Trousers, The)  [s: 0.01, c: 0.80, l: 15.39]\n",
      "(Reservoir Dogs; Star Wars: Episode V; Godfather: Part II, The; American Beauty) => (Star Wars: Episode IV; Pulp Fiction; Godfather, The)  [s: 0.01, c: 0.80, l: 15.39]\n",
      "(Reservoir Dogs; Star Wars: Episode V; Raiders of the Lost Ark; Godfather: Part II, The) => (Star Wars: Episode IV; Pulp Fiction; Godfather, The)  [s: 0.01, c: 0.80, l: 15.34]\n",
      "(Star Wars: Episode IV; Star Wars: Episode V; Fistful of Dollars, A) => (Good, The Bad and The Ugly, The)  [s: 0.01, c: 0.82, l: 15.27]\n",
      "(Godfather, The; Fistful of Dollars, A) => (Good, The Bad and The Ugly, The)  [s: 0.01, c: 0.81, l: 15.07]\n",
      "(Star Wars: Episode V; Fistful of Dollars, A) => (Good, The Bad and The Ugly, The)  [s: 0.01, c: 0.81, l: 14.96]\n",
      "\n",
      "CPU times: user 1min 1s, sys: 628 ms, total: 1min 1s\n",
      "Wall time: 1min 1s\n"
     ]
    }
   ],
   "execution_count": 37
  },
  {
   "cell_type": "markdown",
   "id": "b376c706",
   "metadata": {},
   "source": [
    "**Optional:** Feel free to uncomment and run the code cell below. It uses your implementation of the Apriori algorithm using the same parameters as Run C. You can use this code to double-check your implementation, but please be aware that it will run longer than the `efficient_apriori` package; although not too long for these parameters. Note that the result will not be in the same format and not sorted, but you can easily eyeball that the results will match the one of Run C above...or at least should :)."
   ]
  },
  {
   "cell_type": "code",
   "id": "a0565a21",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-02T12:19:01.920330Z",
     "start_time": "2024-10-02T12:18:37.184057Z"
    }
   },
   "source": [
    "##%%time\n",
    "\n",
    "rules = find_association_rules(shopping_histories, 0.1, 0.8)\n",
    "\n",
    "for lhs, rhs in rules:\n",
    "   print('{} => {}'.format(lhs, rhs))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1221,) => (858,)\n",
      "(1196, 1198) => (260,)\n",
      "(260, 1210) => (1196,)\n",
      "(1196, 1210) => (260,)\n"
     ]
    }
   ],
   "execution_count": 33
  },
  {
   "cell_type": "markdown",
   "id": "03dc0961",
   "metadata": {},
   "source": [
    "#### 2.2 a) Compare the Runs A-D and Discuss your Observations! (3 Points)\n",
    "\n",
    "You must have noticed numerous differences between the 4 runs A-D. List at least 3 differences you have found. You may want to consider the elapsed time and the resulting association rules. Briefly explain your observations! For this subtask, you do not need to look at the movie names (`id_map=None`) as your observations are not specific to the context of movie recommendations; at this we will look in 2.2 b)\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cc66569",
   "metadata": {},
   "source": [
    "1. Lower `min_support` and `min_confidence` values increase the time complexity of the algorithm. This is evident from the comparison of Runs A and B, where Run B has lower values for these parameters and takes longer to execute. This is because the algorithm has to consider more itemsets and rules with lower support and confidence thresholds, respectively.\n",
    "2. Higher `min_confidence` values result in fewer rules being generated. This is evident from the comparison of Runs A and C, where Run C has a higher value for this parameter and generates fewer rules. This is because the algorithm filters out rules with lower confidence, resulting in a smaller set of rules.\n",
    "3. The order of the rules changes based on the metric used for sorting. This is evident from the comparison of Runs C and D, where Run D uses the `reverse=True` parameter to sort the rules in descending order of lift. This changes the order of the rules compared to Run C, where the rules are sorted in ascending order of lift."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d1bc62c",
   "metadata": {},
   "source": [
    "Now run the code cells above for Runs A-D again, but this time with `id_map=movie_map` so that the output will show for each rule the actual movie names.\n",
    "\n",
    "#### 2.2 b) Compare the Runs A-D and discuss the results for building a recommendation engine! (3 Points)\n",
    "\n",
    "Comparing the results of the different runs again, but now seeing the actual movie names, should give you some further insights how the choice of `min_support` and `min_confidence` might affect how the resulting rules are useful for building a recommendation engine.\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d90e0931",
   "metadata": {},
   "source": "In A and C, the rules are simple, which movies are linked to each other. While in B and D the number of rules are increased, which means the rules are more complex and have more items in the rules. This is because the lower `min_support` and `min_confidence` values allow for more rules to be generated, which can capture more complex patterns in the data. The rules generated in A and C are more straightforward and may be more intuitive for users to understand, while the rules in B and D may capture more nuanced relationships between movies. Depending on the use case, the choice of `min_support` and `min_confidence` values can impact the complexity and interpretability of the rules generated, which can affect the usefulness of the recommendation engine."
  },
  {
   "cell_type": "markdown",
   "id": "64b84665",
   "metadata": {},
   "source": [
    "#### 2.2 c) Sketch a Movie Recommendation Algorithm Based on ARM (4 Points)\n",
    "\n",
    "So far, we only looked at individual rules and how the set of rules changes for different parameter values for `min_support` and `min_confidence`. However, we still need some method like `make_recommendation(shopping_history)` that takes the shopping history of a user and returns 1 or more recommendations. The goal is here is *not* to implement such a method but outline the main concerns to consider when implementing such a method\n",
    "\n",
    "\n",
    "(Hint: Do not forget that you not only have the information about Association Rules but also about the individual Frequent Itemsets)\n",
    "\n",
    "**Your Answer:**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7df19f8b",
   "metadata": {},
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "Input (User's Watch History): The function make_recommendation(watch_history) would take the user's watch history as input. This history represents the movies the user has already watched, which will be matched against frequent itemsets and association rules generated from the overall dataset.\n",
    "\n",
    "Use of Frequent Itemsets: Frequent itemsets represent combinations of movies that are often watched together. The algorithm should first attempt to match the user's watch history with frequent itemsets. If the user's watch history overlaps with an itemset, the algorithm can recommend the remaining items (movies) from that itemset that the user has not yet watched.\n",
    "\n",
    "Leveraging Association Rules: Association rules capture correlations between movies, indicating that if a user watches a set of movies (antecedent), they are likely to watch another set (consequent). The algorithm should check the user's watch history against the antecedents of the association rules.\n",
    "If the user's history matches the antecedent of a rule, the algorithm can recommend the movies in the consequent. The confidence of the rule can be used to prioritize the recommendations, with higher confidence rules being more likely to provide relevant recommendations.\n",
    "\n",
    "Ranking Recommendations by Confidence and Support: The recommendations should be ranked or filtered based on the confidence and support of the association rules. Rules with higher confidence suggest a stronger likelihood that the user will watch the recommended movies. Similarly, frequent itemsets with higher support represent more common movie-watching patterns and may be more relevant for the user."
   ],
   "id": "eb75102cbed08282"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8663eead",
   "metadata": {},
   "outputs": [],
   "source": ""
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
